{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ViniciusTavaresSousa/3D-DCGAN/blob/main/3D_DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3D DCGAN**"
      ],
      "metadata": {
        "id": "AXJkHiHs3ZhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UQUZLMBXG01m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Google Drive**"
      ],
      "metadata": {
        "id": "USA1quqd3MVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ibd-ojJq3APh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKkWLCoF5-mg"
      },
      "source": [
        "##**Base de Dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R_ohRTxDpFHn"
      },
      "outputs": [],
      "source": [
        "voxels_treinamento = np.load(\"/content/drive/MyDrive/3D DCGAN/airplane.npy\", allow_pickle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rofWaORckJJl",
        "outputId": "e697fbf4-22a9-4dc3-f072-9ab70ac60bce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(451, 64, 64, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "voxels_treinamento.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VrYgMRRqpQmH"
      },
      "outputs": [],
      "source": [
        "buffer_size = 451\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pgz9s_txpYqo"
      },
      "outputs": [],
      "source": [
        "# Cria um conjunto de dados a partir dos voxels, embaralha e divide em lotes\n",
        "base_de_dados = tf.data.Dataset.from_tensor_slices(voxels_treinamento).shuffle(buffer_size).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681-_nFBH0U2"
      },
      "source": [
        "##**Gerador**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb2wiRHfC9jL"
      },
      "outputs": [],
      "source": [
        "def criaGerador():\n",
        "  modelo = tf.keras.Sequential()\n",
        "\n",
        "  modelo.add(layers.Dense(4*4*4*512, use_bias = False, input_shape = (150,)))\n",
        "  modelo.add(layers.BatchNormalization())\n",
        "  modelo.add(layers.LeakyReLU())\n",
        "\n",
        "  modelo.add(layers.Reshape((4, 4, 4, 512)))\n",
        "\n",
        "  modelo.add(layers.Conv3DTranspose(256, (4, 4, 4), strides = (2, 2, 2), padding = 'same', use_bias = False))\n",
        "  modelo.add(layers.BatchNormalization())\n",
        "  modelo.add(layers.LeakyReLU())\n",
        "\n",
        "  modelo.add(layers.Conv3DTranspose(128, (4, 4, 4), strides = (2, 2, 2), padding = 'same', use_bias = False))\n",
        "  modelo.add(layers.BatchNormalization())\n",
        "  modelo.add(layers.LeakyReLU())\n",
        "\n",
        "  modelo.add(layers.Conv3DTranspose(64, (4, 4, 4), strides = (2, 2, 2), padding = 'same', use_bias = False))\n",
        "  modelo.add(layers.BatchNormalization())\n",
        "  modelo.add(layers.LeakyReLU())\n",
        "\n",
        "  modelo.add(layers.Conv3DTranspose(1, (4, 4, 4), strides = (2, 2, 2), padding = 'same', use_bias = False, activation = 'sigmoid'))\n",
        "\n",
        "  modelo.summary()\n",
        "\n",
        "  return modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibsTZZZyhWCJ"
      },
      "outputs": [],
      "source": [
        "gerador = criaGerador()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmfFYTaDNKOm"
      },
      "source": [
        "##**Discriminador**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beJxc46iNP4l"
      },
      "outputs": [],
      "source": [
        "def criaDiscriminador():\n",
        "  modelo = tf.keras.Sequential()\n",
        "\n",
        "  modelo.add(layers.Conv3D(16, (4, 4, 4), strides = (2, 2, 2), padding = 'same', input_shape = [64, 64, 64, 1]))\n",
        "  modelo.add(layers.LeakyReLU())\n",
        "  modelo.add(layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "  modelo.add(layers.Conv3D(32, (4, 4, 4), strides = (2, 2, 2), padding = 'same'))\n",
        "  modelo.add(layers.LeakyReLU())\n",
        "  modelo.add(layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "  modelo.add(layers.Conv3D(64, (4, 4, 4), strides = (2, 2, 2), padding = 'same'))\n",
        "  modelo.add(layers.LeakyReLU())\n",
        "  modelo.add(layers.Dropout(0.3))\n",
        "\n",
        "  modelo.add(layers.Conv3D(128, (4, 4, 4), strides = (2, 2, 2), padding = 'same'))\n",
        "  modelo.add(layers.LeakyReLU())\n",
        "  modelo.add(layers.Dropout(0.3))\n",
        "\n",
        "  modelo.add(layers.Flatten())\n",
        "  modelo.add(layers.Dense(1))\n",
        "\n",
        "  modelo.summary()\n",
        "\n",
        "  return modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B7XCLdRT5cG"
      },
      "outputs": [],
      "source": [
        "discriminador = criaDiscriminador()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWiNumOFPNi9"
      },
      "source": [
        "##**Função de Perda**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx3cOhAZPVs9"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVWX38rVPknV"
      },
      "outputs": [],
      "source": [
        "def perdaDiscriminador(saida_real, saida_falsa):\n",
        "\n",
        "  saida_real = cross_entropy(tf.ones_like(saida_real), saida_real)\n",
        "  saida_falsa = cross_entropy(tf.zeros_like(saida_falsa), saida_falsa)\n",
        "  perda_total = saida_real + saida_falsa\n",
        "\n",
        "  return perda_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlKRWAARQSHl"
      },
      "outputs": [],
      "source": [
        "def perdaGerador(saida_falsa):\n",
        "\n",
        "  return cross_entropy(tf.ones_like(saida_falsa), saida_falsa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vSwdgDiQfPl"
      },
      "outputs": [],
      "source": [
        "# Inicializa os otimizadores\n",
        "otimizador_gerador = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
        "otimizador_discriminador = tf.keras.optimizers.Adam(learning_rate = 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GetCWoB4Q4_H"
      },
      "outputs": [],
      "source": [
        "# Diretório para checkpoints do treinamento\n",
        "checkpoint_dir = \"/content/drive/MyDrive/3D DCGAN\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(otimizador_gerador = otimizador_gerador,\n",
        "                                 otimizador_discriminador= otimizador_discriminador,\n",
        "                                 gerador = gerador,\n",
        "                                 discriminador = discriminador)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SnP0eSVSfN2"
      },
      "source": [
        "##**Treinamento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7eyqaNQSkAN"
      },
      "outputs": [],
      "source": [
        "epocas = 1630 # Número de épocas de treinamento\n",
        "dim_ruido = 150 # Dimensão do vetor de ruído\n",
        "num_de_exemplos_gerados = 1 # Número de exemplos a gerar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diretório para armazenar os logs do TensorBoard\n",
        "log_dir = \"/content/drive/MyDrive/3D DCGAN/logs/gan/\" + str(batch_size) + \"_\" + str(dim_ruido) + \"_\" + str(epocas)\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)"
      ],
      "metadata": {
        "id": "t4mbIvp0XOsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "entSr_C6SuWF"
      },
      "outputs": [],
      "source": [
        "# Vetor latente para geração de voxels\n",
        "seed = tf.random.normal([num_de_exemplos_gerados, dim_ruido])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqITp1D5S1aV"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def passos_treinamento(voxels, epoca):\n",
        "    ruido = tf.random.normal([batch_size, dim_ruido])\n",
        "\n",
        "    with tf.GradientTape() as tape_gerador, tf.GradientTape() as tape_discriminador:\n",
        "        voxels_gerados = gerador(ruido, training=True)\n",
        "\n",
        "        # Avalia as saídas do discriminador\n",
        "        saida_real = discriminador(voxels, training=True)\n",
        "        saida_falsa = discriminador(voxels_gerados, training=True)\n",
        "\n",
        "        # Calcula a perda\n",
        "        perda_ger = perdaGerador(saida_falsa)\n",
        "        perda_disc = perdaDiscriminador(saida_real, saida_falsa)\n",
        "\n",
        "    # Calcula os gradientes\n",
        "    gradientes_gerador = tape_gerador.gradient(perda_ger, gerador.trainable_variables)\n",
        "    gradientes_discriminador = tape_discriminador.gradient(perda_disc, discriminador.trainable_variables)\n",
        "\n",
        "    # Aplica os gradientes\n",
        "    otimizador_gerador.apply_gradients(zip(gradientes_gerador, gerador.trainable_variables))\n",
        "    otimizador_discriminador.apply_gradients(zip(gradientes_discriminador, discriminador.trainable_variables))\n",
        "\n",
        "    # Registra as perdas no TensorBoard\n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('Perda do Gerador', perda_ger, step=epoca)\n",
        "        tf.summary.scalar('Perda do Discriminador', perda_disc, step=epoca)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1taCskPQp9gU"
      },
      "outputs": [],
      "source": [
        "def gerar_e_salvar_voxels(modelo, epoca, entrada_teste):\n",
        "    # Gera os voxels\n",
        "    predicoes = modelo(entrada_teste, training=False)\n",
        "\n",
        "    # Salva cada voxel gerado\n",
        "    for i in range(predicoes.shape[0]):\n",
        "        voxel = predicoes[i].numpy()\n",
        "        np.save(f\"/content/drive/MyDrive/3D DCGAN/voxels/voxel_{epoca:04d}_{i}.npy\", voxel)\n",
        "\n",
        "    print(f'Voxels salvos para a época {epoca}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZOnmJaRp0VW"
      },
      "outputs": [],
      "source": [
        "def treinar(conjunto_dados, epocas):\n",
        "    for epoca in range(epocas):\n",
        "        inicio = time.time()\n",
        "        for lote_voxels in conjunto_dados:\n",
        "            passos_treinamento(lote_voxels, epoca)\n",
        "\n",
        "        gerar_e_salvar_voxels(gerador, epoca + 1, seed)\n",
        "\n",
        "        if (epoca + 1) % 15 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "        print(f\"Tempo para a época {epoca + 1} em {time.time() - inicio:.2f} segundos\")\n",
        "\n",
        "    gerar_e_salvar_voxels(gerador, epoca + 1, seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/3D\\ DCGAN/logs/gan/"
      ],
      "metadata": {
        "id": "2Eo966NSYTTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHQA-ioUp_OW"
      },
      "outputs": [],
      "source": [
        "# Inicia o treinamento\n",
        "treinar(base_de_dados, epocas)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GKkWLCoF5-mg",
        "681-_nFBH0U2",
        "rmfFYTaDNKOm",
        "TWiNumOFPNi9"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}